version: '3.8'

services:
  job_scrap_db:
    image: postgres:17
    container_name: job_scrap_db
    environment:
      - POSTGRES_DB=job_scrap_db
      - POSTGRES_USER=job_scrap_user
      - POSTGRES_PASSWORD=job_scrap_password
    ports:
      - 5432:5432
    volumes:                           
      - postgres-job_scrap-volume:/var/lib/postgresql/data

  # job_scraper:
  #   image: job_scraper
  #   container_name: job_scraper
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #   # command: bash -c "/job_scraper/scripts/run.sh"
  #   volumes:
  #     - .:/job_scraper
  #   depends_on:
  #     - job_scrap_db

  # airflow:
  #   image: apache/airflow:2.8.1
  #   container_name: airflow
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  #     - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://job_scrap_user:job_scrap_password@job_scrap_db:5432/job_scrap_db
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #   depends_on:
  #     - job_scrap_db
  #   ports:
  #     - 8080:8080
  #   volumes:
  #     - ./airflow/dags:/opt/airflow/dags
  #     - ./airflow/logs:/opt/airflow/logs
  #     - ./airflow/plugins:/opt/airflow/plugins
  #     - ./Pracuj_scrap:/opt/airflow/dags/Pracuj_scrap
  #   command: >
  #     bash -c "airflow db upgrade &&
  #             python /scripts/load_env_to_airflow_variables.py &&
  #              airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com &&
  #              airflow webserver"    
  # airflow_scheduler:
  #   image: apache/airflow:2.8.1
  #   container_name: airflow_scheduler
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  #     - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://job_scrap_user:job_scrap_password@job_scrap_db:5432/job_scrap_db
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #   depends_on:
  #     - job_scrap_db
  #   volumes:
  #     - ./airflow/dags:/opt/airflow/dags
  #     - ./airflow/logs:/opt/airflow/logs
  #     - ./airflow/plugins:/opt/airflow/plugins
  #     - ./Pracuj_scrap:/opt/airflow/dags/Pracuj_scrap
  #   command: >
  #     bash -c "airflow db upgrade &&
  #              airflow scheduler"
    
  pyspark:
    image: jupyter/pyspark-notebook:latest
    container_name: pyspark
    ports:
      - 8888:8888
    environment:
      - PYSPARK_PYTHON=python3
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./Pracuj_scrap:/home/jovyan/work/Pracuj_scrap
    depends_on:
      - job_scrap_db 
volumes:                              
  postgres-job_scrap-volume: {}
